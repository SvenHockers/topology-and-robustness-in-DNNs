# =====================================================================
# Robustness pipeline - Experiment configuration
# ---------------------------------------------------------------------
# Research Question: Do topological changes in neural activations
# correlate with model robustness? Specifically, we test whether:
# - Robust models: small input perturbations → small topological changes
# - Brittle models: small perturbations → large topological changes
#
# Experimental Design:
# 1. Generate synthetic point cloud dataset (circles, spheres, tori)
# 2. Train/load MLP model to classify shapes
# 3. Apply perturbations (adversarial PGD, geometric transforms)
# 4. Compute persistence diagrams on activations (H0, H1) for clean vs perturbed
# 5. Measure Wasserstein distances between diagrams → topology sensitivity
# 6. Correlate topology sensitivity with robustness metrics (RA@eta, eta*)
#
# Key Parameters:
# - Adversarial probes: measure minimal attack radius (eta*) and robust accuracy
# - Layerwise topology: compare clean vs perturbed diagrams at specific ε values
# - Geometric probes: test sensitivity to rotation, translation, jitter, dropout
# - Topology settings: zscore normalization + PCA(16) for stable comparisons
#
# Run with: python scripts/run_robustness.py --config configs/experiment.yaml
# =====================================================================

general:
  seed: 42
  exp_name: "mlp_experiment"
  device: "auto"
  output_dir: "outputs/robustness/mlp_experiment"
  sample_limit: null
  style: "default"

data:
  n_samples_per_shape: 200
  n_points: 20
  noise: 0.1
  val_split: 0.2
  batch_size: 32

model:
  arch: "MLP"
  train: true
  epochs: 20
  lr: 1e-3
  checkpoint: null

probes:
  adversarial:
    enabled: true
    norms: ["linf", "l2"]
    eps_max: 1.0
    steps: 40
    tol: 1e-3
    outer_bisect: true
    eps_grid: [0.0, 0.05, 0.1, 0.2, 0.3, 0.5, 1.0]

  geometric:
    enabled: true
    tol: 1e-3
    rotation:
      axes: ["z"]
      deg_range: [-45, 45]
    translation:
      axes: ["x", "y", "z"]
      range: [-0.2, 0.2]
    jitter:
      std_range: [0.0, 0.2]
      clip: 0.5
    dropout:
      ratio_range: [0.0, 0.5]

  interpolation:
    enabled: true
    pairs_per_class: 20
    match: "index"
    steps: 50
    cross_class: false

  topology:
    enabled: true
    compute_dgm: true
    maxdim: 1
    sample_size: 200
    normalize: "zscore"
    pca_dim: 16
    batches_for_topology: 1
    bootstrap_repeats: 1

  layerwise_topology:
    enabled: true
    layers: ["input", "fc1", "fc2", "fc3", "pooled"]
    distances: ["wasserstein"]
    maxdim: 1
    sample_size: 200
    conditions:
      # Adversarial perturbations: measure topology changes at specific epsilon values
      # These should align with the norms being probed (linf, l2)
      adv_linf_eps: [0.2, 0.4]  # L∞ adversarial perturbations
      adv_l2_eps: [0.2, 0.4]    # L2 adversarial perturbations (L2 typically uses larger eps)
      # Geometric perturbations: measure topology sensitivity to transformations
      rotation_deg: [10, 20]     # Rotation angles (degrees)
      jitter_std: [0.05]         # Gaussian jitter standard deviation
      # Note: translation and dropout probes are enabled but not included here
      # Add them if you want to measure topology changes for these perturbations:
      # translation_dist: [0.1, 0.15]  # Translation distance
      # dropout_ratio: [0.2, 0.3]       # Point dropout ratio

reporting:
  save_csv: true
  save_plots: true
  save_artifacts: false
  sample_visualizations_per_class: 3

