{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topology-Based Adversarial Detection: Results Analysis\n",
    "\n",
    "This notebook collects and analyzes all experimental results across datasets, epsilon values, and trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "BASE_DIR = Path('.')\n",
    "OUT_DIR = BASE_DIR / 'out'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Collect All Results Across Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_results():\n",
    "    \"\"\"Collect results from all trials across all datasets and configurations.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Dataset configurations\n",
    "    datasets = {\n",
    "        'mnist': {'name': 'MNIST', 'model': 'CNN', 'type': 'Image', 'configs': ['base_e_0.1', 'base_e_0.2', 'base_e_0.3']},\n",
    "        'synthetic_shapes': {'name': 'Synthetic Shapes', 'model': 'CNN', 'type': 'Image', 'configs': ['base_e_0.1', 'base_e_0.2', 'base_e_0.3']},\n",
    "        'tabular': {'name': 'Tabular', 'model': 'MLP', 'type': 'Tabular', 'configs': ['base_e_0.1', 'base_e_0.2', 'base_e_0.3']},\n",
    "        'torus_one_hole': {'name': 'Torus (1 hole)', 'model': 'MLP', 'type': 'Synthetic', 'configs': ['base_torus_one_hole_e_0.1', 'base_torus_one_hole_e_0.2', 'base_torus_one_hole_e_0.3_copy']},\n",
    "        'torus_two_holes': {'name': 'Torus (2 holes)', 'model': 'MLP', 'type': 'Synthetic', 'configs': ['base_torus_one_hole_e_0.1', 'base_torus_one_hole_e_0.2', 'base_torus_one_hole_e_0.3_copy']},\n",
    "        'nested_spheres': {'name': 'Nested Spheres', 'model': 'MLP', 'type': 'Synthetic', 'configs': ['base_torus_one_hole_e_0.1', 'base_torus_one_hole_e_0.2', 'base_torus_one_hole_e_0.3_copy']},\n",
    "        'blobs': {'name': 'Blobs', 'model': 'MLP', 'type': 'Synthetic', 'configs': ['base_torus_one_hole_e_0.1', 'base_torus_one_hole_e_0.2', 'base_torus_one_hole_e_0.3_copy']},\n",
    "    }\n",
    "    \n",
    "    for ds_key, ds_info in datasets.items():\n",
    "        ds_path = OUT_DIR / ds_key\n",
    "        if not ds_path.exists():\n",
    "            continue\n",
    "            \n",
    "        for config in ds_info['configs']:\n",
    "            # Extract epsilon from config name\n",
    "            if '_e_' in config:\n",
    "                eps = config.split('_e_')[-1].replace('_copy', '')\n",
    "            else:\n",
    "                eps = 'N/A'\n",
    "            \n",
    "            # Topology method\n",
    "            config_path = ds_path / config / 'runs' / 'trials'\n",
    "            if config_path.exists():\n",
    "                for trial_dir in sorted(config_path.glob('trial_*')):\n",
    "                    metrics_file = trial_dir / 'metrics' / 'metrics.json'\n",
    "                    if metrics_file.exists():\n",
    "                        with open(metrics_file) as f:\n",
    "                            metrics = json.load(f)\n",
    "                        \n",
    "                        adv_metrics = metrics.get('metrics_adv', {})\n",
    "                        if adv_metrics:\n",
    "                            results.append({\n",
    "                                'dataset': ds_info['name'],\n",
    "                                'dataset_key': ds_key,\n",
    "                                'model': ds_info['model'],\n",
    "                                'data_type': ds_info['type'],\n",
    "                                'method': 'topology',\n",
    "                                'epsilon': float(eps) if eps != 'N/A' else None,\n",
    "                                'trial_id': trial_dir.name,\n",
    "                                'roc_auc': adv_metrics.get('roc_auc'),\n",
    "                                'pr_auc': adv_metrics.get('pr_auc'),\n",
    "                                'fpr_at_tpr95': adv_metrics.get('fpr_at_tpr95'),\n",
    "                                'accuracy': adv_metrics.get('accuracy'),\n",
    "                                'f1': adv_metrics.get('f1'),\n",
    "                                'precision': adv_metrics.get('precision'),\n",
    "                                'recall': adv_metrics.get('recall'),\n",
    "                                'config': config,\n",
    "                                'source_file': str(metrics_file)\n",
    "                            })\n",
    "            \n",
    "            # Baseline method\n",
    "            baseline_path = ds_path / 'baseline' / config / 'runs' / 'trials'\n",
    "            if baseline_path.exists():\n",
    "                for trial_dir in sorted(baseline_path.glob('trial_*')):\n",
    "                    metrics_file = trial_dir / 'metrics' / 'metrics.json'\n",
    "                    if metrics_file.exists():\n",
    "                        with open(metrics_file) as f:\n",
    "                            metrics = json.load(f)\n",
    "                        \n",
    "                        adv_metrics = metrics.get('metrics_adv', {})\n",
    "                        if adv_metrics:\n",
    "                            results.append({\n",
    "                                'dataset': ds_info['name'],\n",
    "                                'dataset_key': ds_key,\n",
    "                                'model': ds_info['model'],\n",
    "                                'data_type': ds_info['type'],\n",
    "                                'method': 'baseline',\n",
    "                                'epsilon': float(eps) if eps != 'N/A' else None,\n",
    "                                'trial_id': trial_dir.name,\n",
    "                                'roc_auc': adv_metrics.get('roc_auc'),\n",
    "                                'pr_auc': adv_metrics.get('pr_auc'),\n",
    "                                'fpr_at_tpr95': adv_metrics.get('fpr_at_tpr95'),\n",
    "                                'accuracy': adv_metrics.get('accuracy'),\n",
    "                                'f1': adv_metrics.get('f1'),\n",
    "                                'precision': adv_metrics.get('precision'),\n",
    "                                'recall': adv_metrics.get('recall'),\n",
    "                                'config': f'baseline/{config}',\n",
    "                                'source_file': str(metrics_file)\n",
    "                            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Collect all results\n",
    "df = collect_all_results()\n",
    "print(f\"Total results collected: {len(df)}\")\n",
    "print(f\"\\nDatasets: {df['dataset'].unique()}\")\n",
    "print(f\"Methods: {df['method'].unique()}\")\n",
    "print(f\"Epsilons: {sorted(df['epsilon'].dropna().unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summary Statistics Per Dataset/Epsilon/Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_summary_stats(df):\n",
    "    \"\"\"Compute mean, std, min, max for each dataset/epsilon/method combination.\"\"\"\n",
    "    \n",
    "    summary = df.groupby(['dataset', 'epsilon', 'method']).agg({\n",
    "        'roc_auc': ['mean', 'std', 'min', 'max', 'count'],\n",
    "        'pr_auc': ['mean', 'std'],\n",
    "        'fpr_at_tpr95': ['mean', 'std'],\n",
    "    }).round(4)\n",
    "    \n",
    "    # Flatten column names\n",
    "    summary.columns = ['_'.join(col).strip() for col in summary.columns.values]\n",
    "    summary = summary.reset_index()\n",
    "    \n",
    "    return summary\n",
    "\n",
    "summary_df = compute_summary_stats(df)\n",
    "print(\"Summary Statistics (all trials aggregated):\")\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best results per dataset/epsilon/method\n",
    "best_results = df.loc[df.groupby(['dataset', 'epsilon', 'method'])['roc_auc'].idxmax()]\n",
    "best_results = best_results[['dataset', 'epsilon', 'method', 'roc_auc', 'pr_auc', 'fpr_at_tpr95', 'trial_id', 'source_file']]\n",
    "print(\"\\nBest Results Per Configuration:\")\n",
    "best_results.sort_values(['dataset', 'epsilon', 'method'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Topology Method: Best Results Per Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter topology method only\n",
    "topo_df = df[df['method'] == 'topology'].copy()\n",
    "\n",
    "# Best topology result per dataset/epsilon\n",
    "topo_best = topo_df.loc[topo_df.groupby(['dataset', 'epsilon'])['roc_auc'].idxmax()]\n",
    "topo_best = topo_best[['dataset', 'epsilon', 'roc_auc', 'pr_auc', 'fpr_at_tpr95', 'trial_id']]\n",
    "\n",
    "# Pivot for table format\n",
    "topo_pivot = topo_best.pivot(index='dataset', columns='epsilon', values='roc_auc').round(3)\n",
    "topo_pivot['Best'] = topo_pivot.max(axis=1)\n",
    "topo_pivot['Best_eps'] = topo_pivot.drop('Best', axis=1).idxmax(axis=1)\n",
    "\n",
    "print(\"Topology Method - Best ROC-AUC Per Dataset/Epsilon:\")\n",
    "topo_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed metrics at best epsilon\n",
    "topo_best_overall = topo_df.loc[topo_df.groupby('dataset')['roc_auc'].idxmax()]\n",
    "topo_best_overall = topo_best_overall[['dataset', 'epsilon', 'roc_auc', 'pr_auc', 'fpr_at_tpr95', 'trial_id', 'source_file']]\n",
    "print(\"\\nTopology Method - Best Overall Per Dataset:\")\n",
    "topo_best_overall.sort_values('dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline Method: Best Results Per Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter baseline method only\n",
    "baseline_df = df[df['method'] == 'baseline'].copy()\n",
    "\n",
    "if len(baseline_df) > 0:\n",
    "    # Best baseline result per dataset/epsilon\n",
    "    baseline_best = baseline_df.loc[baseline_df.groupby(['dataset', 'epsilon'])['roc_auc'].idxmax()]\n",
    "    baseline_best = baseline_best[['dataset', 'epsilon', 'roc_auc', 'pr_auc', 'fpr_at_tpr95', 'trial_id']]\n",
    "    \n",
    "    # Pivot for table format\n",
    "    baseline_pivot = baseline_best.pivot(index='dataset', columns='epsilon', values='roc_auc').round(3)\n",
    "    baseline_pivot['Best'] = baseline_pivot.max(axis=1)\n",
    "    \n",
    "    print(\"Baseline Method - Best ROC-AUC Per Dataset/Epsilon:\")\n",
    "    display(baseline_pivot)\n",
    "    \n",
    "    # Best overall\n",
    "    baseline_best_overall = baseline_df.loc[baseline_df.groupby('dataset')['roc_auc'].idxmax()]\n",
    "    baseline_best_overall = baseline_best_overall[['dataset', 'epsilon', 'roc_auc', 'pr_auc', 'fpr_at_tpr95', 'trial_id', 'source_file']]\n",
    "    print(\"\\nBaseline Method - Best Overall Per Dataset:\")\n",
    "    display(baseline_best_overall.sort_values('dataset'))\n",
    "else:\n",
    "    print(\"No baseline results found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Topology vs Baseline Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare best topology vs best baseline per dataset\n",
    "comparison = []\n",
    "\n",
    "for dataset in df['dataset'].unique():\n",
    "    topo_best_roc = topo_df[topo_df['dataset'] == dataset]['roc_auc'].max()\n",
    "    baseline_best_roc = baseline_df[baseline_df['dataset'] == dataset]['roc_auc'].max() if len(baseline_df[baseline_df['dataset'] == dataset]) > 0 else None\n",
    "    \n",
    "    comparison.append({\n",
    "        'Dataset': dataset,\n",
    "        'Topology Best': round(topo_best_roc, 3) if pd.notna(topo_best_roc) else None,\n",
    "        'Baseline Best': round(baseline_best_roc, 3) if pd.notna(baseline_best_roc) else None,\n",
    "        'Difference': round(topo_best_roc - baseline_best_roc, 3) if pd.notna(topo_best_roc) and pd.notna(baseline_best_roc) else None,\n",
    "        'Winner': 'Topology' if pd.notna(topo_best_roc) and pd.notna(baseline_best_roc) and topo_best_roc > baseline_best_roc else 'Baseline'\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison)\n",
    "print(\"Topology vs Baseline - Best ROC-AUC Comparison:\")\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC-AUC distribution by dataset and method\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "datasets = df['dataset'].unique()\n",
    "for i, dataset in enumerate(datasets):\n",
    "    if i >= len(axes):\n",
    "        break\n",
    "    ax = axes[i]\n",
    "    data = df[df['dataset'] == dataset]\n",
    "    \n",
    "    sns.boxplot(data=data, x='epsilon', y='roc_auc', hue='method', ax=ax)\n",
    "    ax.set_title(dataset)\n",
    "    ax.set_xlabel('Epsilon')\n",
    "    ax.set_ylabel('ROC-AUC')\n",
    "    ax.legend(loc='lower right', fontsize=8)\n",
    "    ax.set_ylim(0.4, 1.05)\n",
    "\n",
    "# Hide unused axes\n",
    "for j in range(i+1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.suptitle('ROC-AUC Distribution by Dataset, Epsilon, and Method', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_auc_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of best ROC-AUC (topology only)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Create pivot table with best ROC-AUC per dataset/epsilon\n",
    "heatmap_data = topo_best.pivot(index='dataset', columns='epsilon', values='roc_auc')\n",
    "\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='RdYlGn', \n",
    "            vmin=0.5, vmax=1.0, ax=ax, cbar_kws={'label': 'ROC-AUC'})\n",
    "ax.set_title('Topology Method: Best ROC-AUC Per Dataset/Epsilon')\n",
    "ax.set_xlabel('Epsilon')\n",
    "ax.set_ylabel('Dataset')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('topology_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial variability - show all trials per config\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "topo_df_sorted = topo_df.copy()\n",
    "topo_df_sorted['config_label'] = topo_df_sorted['dataset'] + ' (ε=' + topo_df_sorted['epsilon'].astype(str) + ')'\n",
    "\n",
    "sns.stripplot(data=topo_df_sorted, x='config_label', y='roc_auc', \n",
    "              alpha=0.6, jitter=True, ax=ax)\n",
    "sns.pointplot(data=topo_df_sorted, x='config_label', y='roc_auc', \n",
    "              color='red', markers='_', scale=1.5, ax=ax, errorbar=None)\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "ax.set_xlabel('Configuration')\n",
    "ax.set_ylabel('ROC-AUC')\n",
    "ax.set_title('Topology Method: Trial Variability (dots=individual trials, red=mean)')\n",
    "ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Random')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('trial_variability.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Separability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature_separability(feature_dir):\n",
    "    \"\"\"Compute Cohen's d for all features in a directory.\"\"\"\n",
    "    features = ['topo_h0_count', 'topo_h0_max_persistence', 'topo_h0_total_persistence',\n",
    "                'topo_h0_l2_persistence', 'topo_h0_entropy',\n",
    "                'topo_h1_count', 'topo_h1_max_persistence', 'topo_h1_total_persistence',\n",
    "                'topo_h1_l2_persistence', 'topo_h1_entropy']\n",
    "    \n",
    "    results = {}\n",
    "    feature_dir = Path(feature_dir)\n",
    "    \n",
    "    for feat in features:\n",
    "        clean_file = feature_dir / f'test_clean__{feat}.npy'\n",
    "        adv_file = feature_dir / f'test_adv__{feat}.npy'\n",
    "        \n",
    "        if clean_file.exists() and adv_file.exists():\n",
    "            clean = np.load(clean_file)\n",
    "            adv = np.load(adv_file)\n",
    "            \n",
    "            clean_mean, clean_std = np.mean(clean), np.std(clean)\n",
    "            adv_mean, adv_std = np.mean(adv), np.std(adv)\n",
    "            \n",
    "            pooled_std = np.sqrt((clean_std**2 + adv_std**2) / 2)\n",
    "            cohens_d = (adv_mean - clean_mean) / pooled_std if pooled_std > 0 else 0\n",
    "            \n",
    "            results[feat] = {\n",
    "                'clean_mean': clean_mean,\n",
    "                'clean_std': clean_std,\n",
    "                'adv_mean': adv_mean,\n",
    "                'adv_std': adv_std,\n",
    "                'cohens_d': cohens_d,\n",
    "                'abs_d': abs(cohens_d)\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Collect feature separability for best trials\n",
    "feature_dirs = {\n",
    "    'MNIST': 'out/mnist/base_e_0.1/runs/trials/trial_000012/raw/features',\n",
    "    'Synthetic Shapes': 'out/synthetic_shapes/base_e_0.2/runs/trials/trial_000006/raw/features',\n",
    "    'Torus (1 hole)': 'out/torus_one_hole/base_torus_one_hole_e_0.2/runs/trials/trial_000002/raw/features',\n",
    "    'Torus (2 holes)': 'out/torus_two_holes/base_torus_one_hole_e_0.1/runs/trials/trial_000010/raw/features',\n",
    "    'Nested Spheres': 'out/nested_spheres/base_torus_one_hole_e_0.2/runs/trials/trial_000009/raw/features',\n",
    "    'Blobs': 'out/blobs/base_torus_one_hole_e_0.2/runs/trials/trial_000004/raw/features',\n",
    "}\n",
    "\n",
    "separability_results = {}\n",
    "for ds_name, feat_dir in feature_dirs.items():\n",
    "    if Path(feat_dir).exists():\n",
    "        separability_results[ds_name] = compute_feature_separability(feat_dir)\n",
    "\n",
    "print(f\"Computed separability for {len(separability_results)} datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separability table\n",
    "sep_rows = []\n",
    "for ds_name, features in separability_results.items():\n",
    "    for feat_name, feat_data in features.items():\n",
    "        sep_rows.append({\n",
    "            'Dataset': ds_name,\n",
    "            'Feature': feat_name,\n",
    "            'Clean Mean': feat_data['clean_mean'],\n",
    "            'Adv Mean': feat_data['adv_mean'],\n",
    "            \"Cohen's d\": feat_data['cohens_d'],\n",
    "            \"|d|\": feat_data['abs_d']\n",
    "        })\n",
    "\n",
    "sep_df = pd.DataFrame(sep_rows)\n",
    "\n",
    "# Pivot to show features as rows, datasets as columns\n",
    "sep_pivot = sep_df.pivot(index='Feature', columns='Dataset', values=\"Cohen's d\").round(2)\n",
    "print(\"Feature Separability (Cohen's d) - Positive = adversarial higher:\")\n",
    "sep_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of feature separability\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Use absolute values for heatmap\n",
    "sep_pivot_abs = sep_df.pivot(index='Feature', columns='Dataset', values=\"|d|\").round(2)\n",
    "\n",
    "sns.heatmap(sep_pivot_abs, annot=True, fmt='.2f', cmap='YlOrRd', \n",
    "            vmin=0, vmax=3, ax=ax, cbar_kws={'label': \"|Cohen's d|\"})\n",
    "ax.set_title(\"Feature Separability: |Cohen's d| Between Clean and Adversarial Samples\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_separability_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean separability per dataset\n",
    "mean_sep = sep_df.groupby('Dataset')['|d|'].mean().round(2)\n",
    "print(\"Mean |Cohen's d| per Dataset:\")\n",
    "print(mean_sep.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Hyperparameter Analysis (topo_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_history_params(history_file):\n",
    "    \"\"\"Load trial parameters from history.json.\"\"\"\n",
    "    with open(history_file) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    trials = []\n",
    "    for t in data.get('trials', []):\n",
    "        params = t.get('params', {})\n",
    "        trials.append({\n",
    "            'trial_id': t.get('trial_id'),\n",
    "            'roc_auc': t.get('metric_value'),\n",
    "            'topo_k': params.get('graph.topo_k'),\n",
    "            'k': params.get('graph.k'),\n",
    "            'topo_pca_dim': params.get('graph.topo_pca_dim'),\n",
    "            'topo_preprocess': params.get('graph.topo_preprocess'),\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(trials)\n",
    "\n",
    "# Load MNIST history\n",
    "mnist_history = load_history_params('out/mnist/base_e_0.1/history.json')\n",
    "print(\"MNIST Trial Parameters:\")\n",
    "mnist_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between topo_k and ROC-AUC\n",
    "if 'topo_k' in mnist_history.columns and mnist_history['topo_k'].notna().any():\n",
    "    valid = mnist_history.dropna(subset=['topo_k', 'roc_auc'])\n",
    "    corr = valid['topo_k'].corr(valid['roc_auc'])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.scatter(valid['topo_k'], valid['roc_auc'], alpha=0.7, s=80)\n",
    "    \n",
    "    # Fit line\n",
    "    z = np.polyfit(valid['topo_k'], valid['roc_auc'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(valid['topo_k'].min(), valid['topo_k'].max(), 100)\n",
    "    ax.plot(x_line, p(x_line), 'r--', alpha=0.7, label=f'r = {corr:.3f}')\n",
    "    \n",
    "    ax.set_xlabel('topo_k (neighborhood size)')\n",
    "    ax.set_ylabel('ROC-AUC')\n",
    "    ax.set_title('MNIST: Effect of Neighborhood Size on Detection Performance')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('topo_k_correlation.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nCorrelation(topo_k, ROC-AUC): {corr:.4f}\")\n",
    "    print(f\"Best: topo_k={valid.loc[valid['roc_auc'].idxmax(), 'topo_k']:.0f}, ROC-AUC={valid['roc_auc'].max():.4f}\")\n",
    "    print(f\"Worst: topo_k={valid.loc[valid['roc_auc'].idxmin(), 'topo_k']:.0f}, ROC-AUC={valid['roc_auc'].min():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Summary Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results to CSV\n",
    "df.to_csv('all_trial_results.csv', index=False)\n",
    "print(\"Saved: all_trial_results.csv\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_df.to_csv('summary_statistics.csv', index=False)\n",
    "print(\"Saved: summary_statistics.csv\")\n",
    "\n",
    "# Save best results\n",
    "best_results.to_csv('best_results.csv', index=False)\n",
    "print(\"Saved: best_results.csv\")\n",
    "\n",
    "# Save comparison\n",
    "comparison_df.to_csv('topology_vs_baseline.csv', index=False)\n",
    "print(\"Saved: topology_vs_baseline.csv\")\n",
    "\n",
    "# Save feature separability\n",
    "sep_df.to_csv('feature_separability.csv', index=False)\n",
    "print(\"Saved: feature_separability.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. LaTeX Table Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate LaTeX table for topology results\n",
    "print(\"LaTeX Table - Topology Method (Best ROC-AUC per Dataset/Epsilon):\")\n",
    "print(\"=\"*70)\n",
    "print(r\"\\begin{table}[h]\")\n",
    "print(r\"\\centering\")\n",
    "print(r\"\\caption{Adversarial detection performance of the topology-based detector (ROC-AUC). Bold indicates best performance per dataset.}\")\n",
    "print(r\"\\label{tab:main_detection}\")\n",
    "print(r\"\\begin{tabular}{llcccc}\")\n",
    "print(r\"\\toprule\")\n",
    "print(r\"\\textbf{Dataset} & \\textbf{Model} & $\\boldsymbol{\\epsilon=0.1}$ & $\\boldsymbol{\\epsilon=0.2}$ & $\\boldsymbol{\\epsilon=0.3}$ & \\textbf{Best} \\\\\")\n",
    "print(r\"\\midrule\")\n",
    "\n",
    "dataset_order = ['MNIST', 'Synthetic Shapes', 'Tabular', 'Torus (1 hole)', 'Torus (2 holes)', 'Nested Spheres', 'Blobs']\n",
    "model_map = {'MNIST': 'CNN', 'Synthetic Shapes': 'CNN', 'Tabular': 'MLP', \n",
    "             'Torus (1 hole)': 'MLP', 'Torus (2 holes)': 'MLP', 'Nested Spheres': 'MLP', 'Blobs': 'MLP'}\n",
    "\n",
    "for ds in dataset_order:\n",
    "    if ds in topo_pivot.index:\n",
    "        row = topo_pivot.loc[ds]\n",
    "        vals = [row.get(0.1, None), row.get(0.2, None), row.get(0.3, None)]\n",
    "        best_val = max([v for v in vals if pd.notna(v)])\n",
    "        \n",
    "        formatted = []\n",
    "        for v in vals:\n",
    "            if pd.notna(v):\n",
    "                if v == best_val:\n",
    "                    formatted.append(f\"\\\\textbf{{{v:.3f}}}\")\n",
    "                else:\n",
    "                    formatted.append(f\"{v:.3f}\")\n",
    "            else:\n",
    "                formatted.append(\"--\")\n",
    "        \n",
    "        print(f\"{ds} & {model_map[ds]} & {formatted[0]} & {formatted[1]} & {formatted[2]} & {best_val:.3f} \\\\\\\\\")\n",
    "\n",
    "print(r\"\\bottomrule\")\n",
    "print(r\"\\end{tabular}\")\n",
    "print(r\"\\end{table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate LaTeX table for detailed metrics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LaTeX Table - Detailed Metrics at Optimal Epsilon:\")\n",
    "print(\"=\"*70)\n",
    "print(r\"\\begin{table}[h]\")\n",
    "print(r\"\\centering\")\n",
    "print(r\"\\caption{Detailed detection metrics at optimal $\\epsilon$ per dataset.}\")\n",
    "print(r\"\\label{tab:detailed_metrics}\")\n",
    "print(r\"\\begin{tabular}{lccccc}\")\n",
    "print(r\"\\toprule\")\n",
    "print(r\"\\textbf{Dataset} & \\textbf{$\\epsilon$} & \\textbf{ROC-AUC} & \\textbf{PR-AUC} & \\textbf{FPR@95} \\\\\")\n",
    "print(r\"\\midrule\")\n",
    "\n",
    "for ds in dataset_order:\n",
    "    row = topo_best_overall[topo_best_overall['dataset'] == ds]\n",
    "    if len(row) > 0:\n",
    "        r = row.iloc[0]\n",
    "        print(f\"{ds} & {r['epsilon']} & {r['roc_auc']:.3f} & {r['pr_auc']:.3f} & {r['fpr_at_tpr95']:.3f} \\\\\\\\\")\n",
    "\n",
    "print(r\"\\bottomrule\")\n",
    "print(r\"\\end{tabular}\")\n",
    "print(r\"\\end{table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total trials analyzed: {len(df)}\")\n",
    "print(f\"Topology trials: {len(topo_df)}\")\n",
    "print(f\"Baseline trials: {len(baseline_df)}\")\n",
    "print(f\"\\nDatasets: {len(df['dataset'].unique())}\")\n",
    "print(f\"Epsilon values: {sorted(df['epsilon'].dropna().unique())}\")\n",
    "\n",
    "print(\"\\nTopology Method Performance Summary:\")\n",
    "for ds in dataset_order:\n",
    "    ds_data = topo_df[topo_df['dataset'] == ds]\n",
    "    if len(ds_data) > 0:\n",
    "        print(f\"  {ds}: {ds_data['roc_auc'].mean():.3f} ± {ds_data['roc_auc'].std():.3f} (n={len(ds_data)})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
